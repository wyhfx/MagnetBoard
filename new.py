#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–°çˆ¬è™«ç³»ç»Ÿ - æ”¯æŒå°é¢å›¾å’Œæ‰€æœ‰å›¾ç‰‡åˆ†ç¦»
"""

import asyncio
import json
import os
import re
import time
from typing import Optional, Dict, List
from pathlib import Path
import httpx
from playwright.async_api import async_playwright
import logging
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from datetime import datetime

# ==================== HTTPå®¢æˆ·ç«¯æ¨¡å— ====================
class CrawlerHttpClient:
    def __init__(self, 
                 max_concurrent: int = 5,
                 timeout: int = 30,
                 proxy: Optional[str] = None,
                 cookies_file: str = "data/cookies.json"):
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        
        # è·å–ä»£ç†é…ç½®
        if proxy:
            self.proxy = proxy
        else:
            # ä»ç¯å¢ƒå˜é‡è·å–ä»£ç†
            self.proxy = os.environ.get('HTTP_PROXY') or os.environ.get('http_proxy')
        
        self.cookies_file = cookies_file
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ä¿®æ­£ä»£ç†åœ°å€ï¼ˆåœ¨loggeråˆ›å»ºä¹‹åï¼‰
        if self.proxy and 'host.docker.internal' in self.proxy:
            self.proxy = self.proxy.replace('host.docker.internal', '192.168.31.85')
            self.logger.info(f"ä¿®æ­£ä»£ç†åœ°å€: {self.proxy}")
        
        self.default_headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
        }
        
        Path(cookies_file).parent.mkdir(parents=True, exist_ok=True)
        
    async def create_client(self) -> httpx.AsyncClient:
        limits = httpx.Limits(max_connections=self.max_concurrent)
        
        client = httpx.AsyncClient(
            http2=True,
            timeout=self.timeout,
            limits=limits,
            headers=self.default_headers,
            proxies=self.proxy if self.proxy else None,
            follow_redirects=True
        )
        
        cookies = await self.load_cookies()
        if cookies:
            client.cookies.update(cookies)
            self.logger.info(f"å·²åŠ è½½ {len(cookies)} ä¸ªcookies")
        
        return client
    
    async def load_cookies(self) -> Dict:
        try:
            if os.path.exists(self.cookies_file):
                with open(self.cookies_file, 'r', encoding='utf-8') as f:
                    cookies_data = json.load(f)
                    cookies = {}
                    for cookie in cookies_data:
                        cookies[cookie.get('name', '')] = cookie.get('value', '')
                    return cookies
        except Exception as e:
            self.logger.warning(f"åŠ è½½cookieså¤±è´¥: {e}")
        return {}
    
    async def save_cookies(self, cookies: List[Dict]):
        try:
            with open(self.cookies_file, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, ensure_ascii=False, indent=2)
            self.logger.info(f"å·²ä¿å­˜ {len(cookies)} ä¸ªcookies")
        except Exception as e:
            self.logger.error(f"ä¿å­˜cookieså¤±è´¥: {e}")
    
    async def collect_cookies_with_playwright(self, target_url: str = "https://sehuatang.org"):
        self.logger.info("å¼€å§‹ä½¿ç”¨Playwrightæ”¶é›†cookies...")
        
        # é…ç½®æµè§ˆå™¨å¯åŠ¨å‚æ•°
        browser_args = []
        if self.proxy:
            browser_args.extend([
                f'--proxy-server={self.proxy}',
                '--ignore-certificate-errors',
                '--no-sandbox',
                '--disable-dev-shm-usage'
            ])
        
        # åœ¨Dockerç¯å¢ƒä¸­å¼ºåˆ¶ä½¿ç”¨headlessæ¨¡å¼
        is_docker = os.path.exists('/.dockerenv')
        headless = is_docker
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=headless,
                args=browser_args
            )
            context = await browser.new_context()
            page = await context.new_page()
            
            try:
                await page.goto(target_url, wait_until="networkidle")
                
                # å¤„ç†18+åŒæ„æŒ‰é’®
                selectors = ["button:has-text('åŒæ„')", "button:has-text('è¿›å…¥')"]
                for selector in selectors:
                    try:
                        await page.wait_for_selector(selector, timeout=5000)
                        await page.click(selector)
                        break
                    except:
                        continue
                
                await page.wait_for_timeout(5000)
                
                cookies = await context.cookies()
                await self.save_cookies(cookies)
                
                return cookies
                
            except Exception as e:
                self.logger.error(f"æ”¶é›†cookieså¤±è´¥: {e}")
                return []
            finally:
                await browser.close()
    
    async def request_with_semaphore(self, client: httpx.AsyncClient, url: str, **kwargs) -> httpx.Response:
        async with self.semaphore:
            try:
                response = await client.get(url, **kwargs)
                return response
            except Exception as e:
                self.logger.error(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
                raise
    
    async def close_client(self, client: httpx.AsyncClient):
        await client.aclose()

# ==================== åˆ—è¡¨é¡µè§£æå™¨ ====================
class ListPageParser:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.base_url = "https://sehuatang.org"
        
        self.themes = {
            "36": {"name": "äºšæ´²æ— ç ", "fid": "36"},
            "37": {"name": "äºšæ´²æœ‰ç ", "fid": "37"},
            "2": {"name": "å›½äº§åŸåˆ›", "fid": "2"},
            "103": {"name": "é«˜æ¸…ä¸­æ–‡å­—å¹•", "fid": "103"},
            "104": {"name": "ç´ äººåŸåˆ›", "fid": "104"},
            "39": {"name": "åŠ¨æ¼«åŸåˆ›", "fid": "39"},
            "152": {"name": "éŸ©å›½ä¸»æ’­", "fid": "152"}
        }
    
    def generate_list_urls(self, fid: str, page: int = 1) -> List[str]:
        urls = []
        
        # æ–¹å¼1: forum-{fid}-{page}.html
        url1 = f"{self.base_url}/forum-{fid}-{page}.html"
        urls.append(url1)
        
        # æ–¹å¼2: forum.php?mod=forumdisplay&fid={fid}&page={page}
        url2 = f"{self.base_url}/forum.php?mod=forumdisplay&fid={fid}&page={page}"
        urls.append(url2)
        
        return urls
    
    def parse_thread_links(self, html: str, source_url: str) -> List[Dict]:
        soup = BeautifulSoup(html, 'html.parser')
        threads = []
        
        selectors = [
            "a[href*='thread-']",
            "a[href*='forum.php?mod=viewthread']",
            ".tl a",
            ".s a",
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                if not href:
                    continue
                
                tid = self.extract_tid(href)
                if not tid:
                    continue
                
                title = self.extract_title(link)
                if not title:
                    continue
                
                # è¿‡æ»¤æ˜æ˜¾æ˜¯å¹¿å‘Šçš„å¸–å­æ ‡é¢˜
                if self.is_advertisement_title(title):
                    continue
                
                full_url = urljoin(source_url, href)
                
                thread_info = {
                    'tid': tid,
                    'title': title,
                    'url': full_url,
                    'source_url': source_url
                }
                
                threads.append(thread_info)
        
        return threads
    
    def extract_tid(self, href: str) -> Optional[str]:
        match = re.search(r'thread-(\d+)', href)
        if match:
            return match.group(1)
        
        match = re.search(r'tid=(\d+)', href)
        if match:
            return match.group(1)
        
        return None
    
    def extract_title(self, link_element) -> Optional[str]:
        title = link_element.get('title', '')
        if title:
            return title.strip()
        
        title = link_element.get_text(strip=True)
        if title:
            return title
        
        for attr in ['alt', 'data-title']:
            title = link_element.get(attr, '')
            if title:
                return title.strip()
        
        return None
    
    def is_advertisement_title(self, title: str) -> bool:
        """æ£€æµ‹æ ‡é¢˜æ˜¯å¦ä¸ºå¹¿å‘Š"""
        if not title:
            return True
        
        title_lower = title.lower()
        
        # å¹¿å‘Šå…³é”®è¯
        ad_keywords = [
            'å¹¿å‘Š', 'æ¨å¹¿', 'èµåŠ©', 'åˆä½œ', 'å•†ä¸š', 'è¥é”€',
            'ad', 'advertisement', 'sponsor', 'promotion', 'commercial',
            'æ¨å¹¿é“¾æ¥', 'å¹¿å‘Šä½', 'æ‹›å•†', 'ä»£ç†', 'åŠ ç›Ÿ',
            'èµšé’±', 'è‡´å¯Œ', 'å…¼èŒ', 'æ‹›è˜', 'æ±‚èŒ',
            'æ¸¸æˆ', 'èµŒåš', 'åšå½©', 'å½©ç¥¨', 'æ—¶æ—¶å½©',
            'è´·æ¬¾', 'ä¿¡ç”¨å¡', 'ç†è´¢', 'æŠ•èµ„', 'è‚¡ç¥¨',
            'ä¿å¥å“', 'å‡è‚¥', 'ç¾å®¹', 'æ•´å½¢', 'å¢é«˜',
            'ä»£è´­', 'ä»£åˆ·', 'ä»£ç»ƒ', 'ä»£å……', 'ä»£æŒ‚',
            'åˆ·å•', 'åˆ·é’»', 'åˆ·ä¿¡èª‰', 'åˆ·æµé‡', 'åˆ·ç²‰ä¸',
            'è‰²æƒ…', 'æˆäºº', 'ä¸€å¤œæƒ…', 'æ´äº¤', 'æŒ‰æ‘©',
            'åŠè¯', 'åˆ»ç« ', 'å‘ç¥¨', 'å‡è¯', 'å‡æ–‡å‡­',
            'é»‘å®¢', 'ç ´è§£', 'ç›—å·', 'åˆ·é’»', 'å¤–æŒ‚'
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹¿å‘Šå…³é”®è¯
        for keyword in ad_keywords:
            if keyword in title_lower:
                return True
        
        # æ£€æŸ¥æ ‡é¢˜é•¿åº¦ï¼ˆå¤ªçŸ­çš„å¯èƒ½æ˜¯å¹¿å‘Šï¼‰
        if len(title.strip()) < 5:
            return True
        
        # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯ç‰¹æ®Šå­—ç¬¦
        if re.match(r'^[^\u4e00-\u9fff\w\s]+$', title):
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤§é‡æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦ï¼ˆå¯èƒ½æ˜¯åƒåœ¾ä¿¡æ¯ï¼‰
        if len(re.findall(r'[0-9]', title)) > len(title) * 0.3:
            return True
        
        return False
    
    def deduplicate_threads(self, threads: List[Dict]) -> List[Dict]:
        seen_tids = set()
        unique_threads = []
        
        for thread in threads:
            tid = thread.get('tid')
            if tid and tid not in seen_tids:
                seen_tids.add(tid)
                unique_threads.append(thread)
        
        self.logger.info(f"å»é‡å‰: {len(threads)} ä¸ªå¸–å­, å»é‡å: {len(unique_threads)} ä¸ªå¸–å­")
        return unique_threads
    
    async def parse_list_page(self, client: httpx.AsyncClient, fid: str, page: int = 1) -> List[Dict]:
        urls = self.generate_list_urls(fid, page)
        all_threads = []
        
        responses = await asyncio.gather(*[
            self.request_page(client, url) for url in urls
        ], return_exceptions=True)
        
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                self.logger.error(f"è¯·æ±‚å¤±è´¥ {urls[i]}: {response}")
                continue
            
            if response and response.status_code == 200:
                threads = self.parse_thread_links(response.text, urls[i])
                all_threads.extend(threads)
                self.logger.info(f"ä» {urls[i]} è§£æåˆ° {len(threads)} ä¸ªå¸–å­")
            else:
                self.logger.warning(f"è¯·æ±‚å¤±è´¥ {urls[i]}: çŠ¶æ€ç  {response.status_code if response else 'None'}")
        
        unique_threads = self.deduplicate_threads(all_threads)
        return unique_threads
    
    async def request_page(self, client: httpx.AsyncClient, url: str) -> Optional[httpx.Response]:
        try:
            response = await client.get(url)
            return response
        except Exception as e:
            self.logger.error(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            return None
    
    def filter_threads_by_keywords(self, threads: List[Dict], keywords: List[str]) -> List[Dict]:
        if not keywords:
            return threads
        
        filtered_threads = []
        for thread in threads:
            title = thread.get('title', '').lower()
            for keyword in keywords:
                if keyword.lower() in title:
                    filtered_threads.append(thread)
                    break
        
        self.logger.info(f"å…³é”®è¯è¿‡æ»¤: {len(threads)} -> {len(filtered_threads)}")
        return filtered_threads
    
    async def parse_thread_detail(self, client: httpx.AsyncClient, thread_url: str) -> Optional[Dict]:
        """è§£æå¸–å­è¯¦æƒ…é¡µé¢ï¼Œæå–ç£åŠ›é“¾æ¥ã€ç•ªå·ç­‰ä¿¡æ¯"""
        try:
            response = await client.get(thread_url)
            if response.status_code != 200:
                self.logger.warning(f"è·å–å¸–å­è¯¦æƒ…å¤±è´¥: {thread_url}, çŠ¶æ€ç : {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–åŸºæœ¬ä¿¡æ¯
            title = self.extract_thread_title(soup)
            content = self.extract_thread_content(soup)
            
            # æå–ç£åŠ›é“¾æ¥
            magnets = self.extract_magnet_links(soup)
            
            # æå–ç•ªå·
            code = self.extract_code(title, content)
            
            # æå–å…¶ä»–ä¿¡æ¯
            author = self.extract_author(soup)
            size = self.extract_size(content)
            is_uncensored = self.check_uncensored(title, content)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç£åŠ›é“¾æ¥ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·³è¿‡
            if not magnets:
                self.logger.info(f"è·³è¿‡æ— ç£åŠ›é“¾æ¥çš„å¸–å­: {thread_url}")
                return None
            
            # æå–å›¾ç‰‡ï¼ˆåˆ†åˆ«æå–å°é¢å›¾å’Œæ‰€æœ‰å›¾ç‰‡ï¼‰
            images = self.extract_images(soup)
            
            thread_detail = {
                'title': title,
                'content': content,
                'magnets': magnets,
                'code': code,
                'author': author,
                'size': size,
                'is_uncensored': is_uncensored,
                'images': images,
                'url': thread_url
            }
            
            return thread_detail
            
        except Exception as e:
            self.logger.error(f"è§£æå¸–å­è¯¦æƒ…å¤±è´¥ {thread_url}: {e}")
            return None
    
    def extract_thread_title(self, soup) -> str:
        """æå–å¸–å­æ ‡é¢˜"""
        title_selectors = [
            'h1#thread_subject',
            'h1.thread_subject',
            'h1',
            '.thread_subject',
            'title'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True)
                if title:
                    return title
        
        return ""
    
    def extract_thread_content(self, soup) -> str:
        """æå–å¸–å­å†…å®¹"""
        content_selectors = [
            'div.t_msgfont',
            'div.postmessage',
            '.t_msgfont',
            '.postmessage',
            'div[id*="post_"]'
        ]
        
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text(strip=True)
                if content:
                    return content
        
        return ""
    
    def extract_magnet_links(self, soup) -> List[str]:
        """æå–ç£åŠ›é“¾æ¥"""
        magnets = []
        
        # æŸ¥æ‰¾ç£åŠ›é“¾æ¥ - ä½¿ç”¨ä¸æ—§ç³»ç»Ÿä¸€è‡´çš„æ­£åˆ™è¡¨è¾¾å¼
        magnet_pattern = r'magnet:\?xt=urn:btih:[0-9A-Fa-f]{40,}'
        
        # ä»é¡µé¢æ–‡æœ¬ä¸­æå–
        page_text = soup.get_text()
        magnets = re.findall(magnet_pattern, page_text, re.IGNORECASE)
        
        # ä»é“¾æ¥ä¸­æå–
        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href', '')
            if href.startswith('magnet:'):
                magnets.append(href)
        
        # å»é‡
        magnets = list(set(magnets))
        return magnets
    
    def extract_code(self, title: str, content: str) -> str:
        """æå–ç•ªå·"""
        # å¸¸è§çš„ç•ªå·æ ¼å¼
        code_patterns = [
            r'[A-Z]{2,4}-\d{3,4}',  # å¦‚ ABC-123, ABCD-1234
            r'[A-Z]{2,4}\d{3,4}',   # å¦‚ ABC123, ABCD1234
            r'[A-Z]{2,4}-\d{2,3}',  # å¦‚ ABC-12, ABCD-123
        ]
        
        text = f"{title} {content}"
        for pattern in code_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                return matches[0].upper()
        
        return ""
    
    def extract_author(self, soup) -> str:
        """æå–å¥³ä¼˜ä¿¡æ¯ - ä»å¸–å­å†…å®¹ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯"""
        # è·å–å¸–å­å†…å®¹æ–‡æœ¬
        content_text = ""
        content_selectors = [
            'div.t_msgfont',
            'div.postmessage',
            '.t_msgfont',
            '.postmessage',
            'div[id*="post_"]',
            'td.t_f'
        ]
        
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content_text = element.get_text(strip=True)
                break
        
        if not content_text:
            return ""
        
        # ä»å†…å®¹ä¸­æå–å¥³ä¼˜ä¿¡æ¯
        # åŒ¹é…ã€å‡ºæ¼”å¥³ä¼˜ã€‘ï¼šå¥³ä¼˜åç§° æ ¼å¼
        actress_patterns = [
            r'ã€å‡ºæ¼”å¥³ä¼˜ã€‘ï¼š\s*([^\n\rã€ã€‘]+)',
            r'ã€å¥³ä¼˜ã€‘ï¼š\s*([^\n\rã€ã€‘]+)',
            r'ã€æ¼”å‘˜ã€‘ï¼š\s*([^\n\rã€ã€‘]+)',
            r'å‡ºæ¼”å¥³ä¼˜[ï¼š:]\s*([^\n\rã€ã€‘]+)',
            r'å¥³ä¼˜[ï¼š:]\s*([^\n\rã€ã€‘]+)',
            r'æ¼”å‘˜[ï¼š:]\s*([^\n\rã€ã€‘]+)'
        ]
        
        for pattern in actress_patterns:
            match = re.search(pattern, content_text)
            if match:
                actress = match.group(1).strip()
                # æ¸…ç†å¥³ä¼˜åç§°
                actress = re.sub(r'[^\u4e00-\u9fff\w\s]', '', actress).strip()
                if actress and len(actress) > 1:
                    return actress
        
        return ""
    
    def extract_size(self, content: str) -> str:
        """æå–æ–‡ä»¶å¤§å° - ä»å¸–å­å†…å®¹ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯"""
        # åŒ¹é…ã€å½±ç‰‡å®¹é‡ã€‘ï¼šå¤§å° æ ¼å¼
        size_patterns = [
            r'ã€å½±ç‰‡å®¹é‡ã€‘ï¼š\s*(\d+(?:\.\d+)?)\s*(GB|MB|KB|G|M|K)',
            r'ã€å®¹é‡ã€‘ï¼š\s*(\d+(?:\.\d+)?)\s*(GB|MB|KB|G|M|K)',
            r'å½±ç‰‡å®¹é‡[ï¼š:]\s*(\d+(?:\.\d+)?)\s*(GB|MB|KB|G|M|K)',
            r'å®¹é‡[ï¼š:]\s*(\d+(?:\.\d+)?)\s*(GB|MB|KB|G|M|K)',
            # å¤‡ç”¨æ¨¡å¼ï¼šç›´æ¥åŒ¹é…å¤§å°
            r'(\d+(?:\.\d+)?)\s*(GB|MB|KB|G|M|K)B?'
        ]
        
        for pattern in size_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                size, unit = matches[0]
                # æ ‡å‡†åŒ–å•ä½
                unit = unit.upper()
                if unit in ["G", "GB"]:
                    return f"{size}GB"
                elif unit in ["M", "MB"]:
                    return f"{size}MB"
                elif unit in ["K", "KB"]:
                    return f"{size}KB"
        
        return ""
    
    def check_uncensored(self, title: str, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ— ç  - ä»å¸–å­å†…å®¹ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯"""
        # é¦–å…ˆå°è¯•ä»ç»“æ„åŒ–ä¿¡æ¯ä¸­æå–
        text = f"{title} {content}"
        
        # åŒ¹é…ã€æ˜¯å¦æœ‰ç ã€‘ï¼šæœ‰ç /æ— ç  æ ¼å¼
        censored_patterns = [
            r'ã€æ˜¯å¦æœ‰ç ã€‘ï¼š\s*(æ— ç |æœ‰ç )',
            r'ã€æœ‰ç ã€‘ï¼š\s*(æ— ç |æœ‰ç )',
            r'æ˜¯å¦æœ‰ç [ï¼š:]\s*(æ— ç |æœ‰ç )',
            r'æœ‰ç [ï¼š:]\s*(æ— ç |æœ‰ç )'
        ]
        
        for pattern in censored_patterns:
            match = re.search(pattern, text)
            if match:
                status = match.group(1)
                return status == "æ— ç "
        
        # å¤‡ç”¨æ£€æµ‹ï¼šå…³é”®è¯åŒ¹é…
        uncensored_keywords = [
            "æ— ç ", "ç„¡ç¢¼", "uncensored", "æ— ä¿®æ­£", "ç„¡ä¿®æ­£",
            "æµå‡º", "ç ´è§£", "ç ´è§£ç‰ˆ", "ç ´è§£ç‰ˆæµå‡º"
        ]
        
        text_lower = text.lower()
        for keyword in uncensored_keywords:
            if keyword.lower() in text_lower:
                return True
        
        return False
    
    def extract_images(self, soup) -> Dict[str, List[str]]:
        """æå–å›¾ç‰‡é“¾æ¥ - åˆ†åˆ«æå–å°é¢å›¾å’Œæ‰€æœ‰å›¾ç‰‡"""
        all_images = []  # æ‰€æœ‰å›¾ç‰‡
        cover_images = []  # å°é¢å›¾åˆ—è¡¨
        
        # 1. æå–æ‰€æœ‰å›¾ç‰‡
        content_selectors = [
            'td.t_f img',  # æ—§ç³»ç»Ÿä½¿ç”¨çš„ä¸»è¦é€‰æ‹©å™¨
            'div.t_msgfont img',
            'div.postmessage img',
            '.t_msgfont img',
            '.postmessage img',
            'div[id*="post_"] img'
        ]
        
        for selector in content_selectors:
            imgs = soup.select(selector)
            for img in imgs:
                src = img.get('src') or img.get('data-src') or img.get('zoomfile') or img.get('file')
                if src:
                    # å¤„ç†ç›¸å¯¹URL
                    if not src.startswith('http'):
                        src = f"https://sehuatang.org{src}" if src.startswith('/') else f"https://sehuatang.org/{src}"
                    
                    # åªä¿ç•™jpgæ ¼å¼çš„å›¾ç‰‡ï¼Œå¹¶è¿‡æ»¤å¹¿å‘Š
                    if (src.lower().endswith('.jpg') or src.lower().endswith('.jpeg')) and (
                        'none.gif' not in src and 
                        'placeholder' not in src and
                        'static/image/common' not in src and
                        'avatar' not in src and
                        'logo' not in src and
                        'icon' not in src and
                        'btn' not in src and
                        'torrent.gif' not in src and  # æ’é™¤ç§å­æ–‡ä»¶å›¾æ ‡
                        'ad' not in src.lower() and  # æ’é™¤å¹¿å‘Š
                        'banner' not in src.lower() and  # æ’é™¤æ¨ªå¹…
                        'sponsor' not in src.lower() and  # æ’é™¤èµåŠ©
                        'ads' not in src.lower() and  # æ’é™¤å¹¿å‘Š
                        'advertisement' not in src.lower() and  # æ’é™¤å¹¿å‘Š
                        'promo' not in src.lower() and  # æ’é™¤æ¨å¹¿
                        'commercial' not in src.lower() and  # æ’é™¤å•†ä¸šå¹¿å‘Š
                        'tuiguang' not in src.lower() and  # æ’é™¤æ¨å¹¿
                        'guanggao' not in src.lower()  # æ’é™¤å¹¿å‘Š
                    ):
                        if src not in all_images:
                            all_images.append(src)
        
        # 2. æŸ¥æ‰¾é™„ä»¶é“¾æ¥ä¸­çš„å›¾ç‰‡
        content_elements = soup.select('td.t_f, div.t_msgfont, div.postmessage')
        for content_elem in content_elements:
            attachment_links = content_elem.find_all('a', href=True)
            for link in attachment_links:
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                # åªæŸ¥æ‰¾jpgæ ¼å¼çš„å›¾ç‰‡é™„ä»¶é“¾æ¥
                if href.lower().endswith('.jpg') or href.lower().endswith('.jpeg'):
                    if not href.startswith('http'):
                        href = f"https://sehuatang.org{href}" if href.startswith('/') else f"https://sehuatang.org/{href}"
                    if href not in all_images:
                        all_images.append(href)
        
        # 3. ä»æ‰€æœ‰å›¾ç‰‡ä¸­ç­›é€‰å°é¢å›¾
        for img_url in all_images:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å°é¢å›¾ï¼ˆæ–‡ä»¶åæ ¼å¼ï¼‰
            if re.search(r'[A-Z]{2,4}-\d{3,4}', img_url, re.IGNORECASE):
                if img_url not in cover_images:
                    cover_images.append(img_url)
        
        # 4. è¿”å›ç»“æœ
        return {
            "cover_images": cover_images[:2],  # å°é¢å›¾æœ€å¤š2å¼ 
            "all_images": all_images  # æ‰€æœ‰å›¾ç‰‡
        }
    
    def get_theme_info(self, fid: str) -> Optional[Dict]:
        return self.themes.get(fid)
    
    def get_all_theme_ids(self) -> List[str]:
        return list(self.themes.keys())

# ==================== ä¸»çˆ¬è™«æ§åˆ¶å™¨ ====================
class NewCrawlerController:
    def __init__(self, 
                 max_concurrent: int = 5,
                 proxy: Optional[str] = None,
                 cookies_file: str = "data/cookies.json",
                 save_dir: str = "data/crawler_results"):
        self.max_concurrent = max_concurrent
        self.proxy = proxy
        self.cookies_file = cookies_file
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        self.http_client = CrawlerHttpClient(
            max_concurrent=max_concurrent,
            proxy=proxy,
            cookies_file=cookies_file
        )
        self.parser = ListPageParser()
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        self.progress_callback = None
        self.log_callback = None
        self.is_running = False
        
    def set_progress_callback(self, callback):
        self.progress_callback = callback
    
    def set_log_callback(self, callback):
        self.log_callback = callback
    
    def add_log(self, message: str, level: str = "INFO"):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {level}: {message}"
        
        if level == "INFO":
            self.logger.info(message)
        elif level == "ERROR":
            self.logger.error(message)
        elif level == "WARNING":
            self.logger.warning(message)
        
        if self.log_callback:
            try:
                self.log_callback(timestamp, level, message)
            except Exception as e:
                self.logger.error(f"æ—¥å¿—å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
    
    async def collect_cookies(self) -> bool:
        try:
            self.add_log("å¼€å§‹æ”¶é›†cookies...")
            cookies = await self.http_client.collect_cookies_with_playwright()
            if cookies:
                self.add_log(f"æˆåŠŸæ”¶é›† {len(cookies)} ä¸ªcookies")
                return True
            else:
                self.add_log("cookiesæ”¶é›†å¤±è´¥", "ERROR")
                return False
        except Exception as e:
            self.add_log(f"æ”¶é›†cookieså¼‚å¸¸: {e}", "ERROR")
            return False
    
    async def crawl_single_theme(self, 
                                fid: str, 
                                start_page: int = 1, 
                                end_page: int = 5,
                                keywords: Optional[List[str]] = None,
                                delay_between_pages: int = 2) -> List[Dict]:
        """çˆ¬å–å•ä¸ªä¸»é¢˜çš„é¡µé¢"""
        if self.is_running:
            self.add_log("çˆ¬è™«æ­£åœ¨è¿è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆ", "WARNING")
            return []
        
        self.is_running = True
        all_threads = []
        
        try:
            client = await self.http_client.create_client()
            
            theme_info = self.parser.get_theme_info(fid)
            theme_name = theme_info.get('name', f'è®ºå›{fid}') if theme_info else f'è®ºå›{fid}'
            
            self.add_log(f"å¼€å§‹çˆ¬å– {theme_name} (FID: {fid})")
            self.add_log(f"çˆ¬å–èŒƒå›´: ç¬¬{start_page}é¡µ - ç¬¬{end_page}é¡µ")
            self.add_log(f"é¡µé¢é—´å»¶è¿Ÿ: {delay_between_pages}ç§’")
            
            total_pages = end_page - start_page + 1
            
            for page in range(start_page, end_page + 1):
                if not self.is_running:
                    self.add_log("çˆ¬è™«è¢«åœæ­¢", "WARNING")
                    break
                
                try:
                    self.add_log(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
                    
                    threads = await self.parser.parse_list_page(client, fid, page)
                    
                    if threads:
                        all_threads.extend(threads)
                        self.add_log(f"ç¬¬ {page} é¡µè§£æåˆ° {len(threads)} ä¸ªå¸–å­")
                    else:
                        self.add_log(f"ç¬¬ {page} é¡µæœªè§£æåˆ°å¸–å­", "WARNING")
                    
                    if self.progress_callback:
                        progress = (page - start_page + 1) / total_pages * 100
                        self.progress_callback(progress, f"ç¬¬ {page} é¡µå®Œæˆ")
                    
                    # é¡µé¢é—´å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                    if page < end_page:
                        self.add_log(f"ç­‰å¾… {delay_between_pages} ç§’åç»§ç»­...")
                        await asyncio.sleep(delay_between_pages)
                    
                except Exception as e:
                    self.add_log(f"çˆ¬å–ç¬¬ {page} é¡µå¤±è´¥: {e}", "ERROR")
                    continue
            
            unique_threads = self.parser.deduplicate_threads(all_threads)
            self.add_log(f"å»é‡åå…± {len(unique_threads)} ä¸ªå¸–å­")
            
            if keywords:
                filtered_threads = self.parser.filter_threads_by_keywords(unique_threads, keywords)
                self.add_log(f"å…³é”®è¯è¿‡æ»¤å {len(filtered_threads)} ä¸ªå¸–å­")
                unique_threads = filtered_threads
            
            # çˆ¬å–å¸–å­è¯¦æƒ…å¹¶ä¿å­˜åˆ°æ•°æ®åº“
            detailed_threads = await self.crawl_thread_details(client, unique_threads, fid)
            
            await self.save_results(detailed_threads, fid, theme_name)
            
            return detailed_threads
            
        except Exception as e:
            self.add_log(f"çˆ¬å–è¿‡ç¨‹å¼‚å¸¸: {e}", "ERROR")
            return []
        finally:
            if 'client' in locals():
                await self.http_client.close_client(client)
            self.is_running = False
    
    async def save_to_database(self, thread: Dict, fid: str):
        """ä¿å­˜å¸–å­ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            from db import SessionLocal
            from models_magnet import MagnetLink
            
            db = SessionLocal()
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = db.query(MagnetLink).filter(
                MagnetLink.magnet_hash == thread.get('tid', '')
            ).first()
            
            if existing:
                self.add_log(f"å¸–å­å·²å­˜åœ¨ï¼Œè·³è¿‡: {thread.get('title', 'Unknown')}")
                db.close()
                return
            
            # å¤„ç†å›¾ç‰‡æ•°æ®
            images_data = thread.get('images', {})
            cover_images = images_data.get('cover_images', [])
            all_images = images_data.get('all_images', [])
            
            # è®¾ç½®å°é¢å›¾URLï¼ˆå–ç¬¬ä¸€å¼ å°é¢å›¾ï¼‰
            cover_url = cover_images[0] if cover_images else None
            
            # åˆ›å»ºæ–°çš„ç£åŠ›é“¾æ¥è®°å½•
            magnet_link = MagnetLink(
                title=thread.get('title', ''),
                content=thread.get('content', ''),
                code=thread.get('code', ''),
                author=thread.get('author', ''),
                size=thread.get('size', ''),
                is_uncensored=thread.get('is_uncensored', False),
                forum_id=fid,
                forum_type=self.parser.get_theme_info(fid).get('name', '') if self.parser.get_theme_info(fid) else '',
                magnet_hash=thread.get('tid', ''),
                url=thread.get('url', ''),
                magnets=json.dumps(thread.get('magnets', []), ensure_ascii=False),
                images=json.dumps(all_images, ensure_ascii=False),  # ä¿å­˜æ‰€æœ‰å›¾ç‰‡
                cover_url=cover_url,  # ä¿å­˜å°é¢å›¾URL
            )
            
            db.add(magnet_link)
            db.commit()
            
            self.add_log(f"ä¿å­˜åˆ°æ•°æ®åº“æˆåŠŸ: {thread.get('code', 'Unknown')} - {thread.get('title', 'Unknown')}")
            
            db.close()
            
        except Exception as e:
            self.add_log(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}", "ERROR")
    
    async def crawl_thread_details(self, client: httpx.AsyncClient, threads: List[Dict], fid: str) -> List[Dict]:
        """çˆ¬å–å¸–å­è¯¦æƒ…å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
        detailed_threads = []
        total_threads = len(threads)
        
        self.add_log(f"å¼€å§‹çˆ¬å– {total_threads} ä¸ªå¸–å­çš„è¯¦ç»†ä¿¡æ¯...")
        
        for i, thread in enumerate(threads):
            if not self.is_running:
                self.add_log("çˆ¬è™«è¢«åœæ­¢", "WARNING")
                break
            
            try:
                thread_url = thread.get('url')
                if not thread_url:
                    continue
                
                self.add_log(f"çˆ¬å–å¸–å­è¯¦æƒ… {i+1}/{total_threads}: {thread.get('title', 'Unknown')}")
                
                # çˆ¬å–å¸–å­è¯¦æƒ…
                detail = await self.parser.parse_thread_detail(client, thread_url)
                
                if detail:
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç£åŠ›é“¾æ¥
                    magnets = detail.get('magnets', [])
                    if not magnets:
                        self.add_log(f"è·³è¿‡æ— ç£åŠ›é“¾æ¥çš„å¸–å­: {thread_url}")
                        continue
                    
                    # åˆå¹¶åŸºæœ¬ä¿¡æ¯
                    thread.update(detail)
                    
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    await self.save_to_database(thread, fid)
                    
                    detailed_threads.append(thread)
                    self.add_log(f"å¸–å­è¯¦æƒ…çˆ¬å–æˆåŠŸ: {detail.get('code', 'Unknown')} - {len(magnets)} ä¸ªç£åŠ›é“¾æ¥")
                else:
                    self.add_log(f"å¸–å­è¯¦æƒ…çˆ¬å–å¤±è´¥: {thread_url}", "WARNING")
                
                # è¿›åº¦å›è°ƒ
                if self.progress_callback:
                    progress = (i + 1) / total_threads * 100
                    self.progress_callback(progress, f"è¯¦æƒ…çˆ¬å–è¿›åº¦: {i+1}/{total_threads}")
                
                # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(1)
                
            except Exception as e:
                self.add_log(f"çˆ¬å–å¸–å­è¯¦æƒ…å¤±è´¥ {thread.get('url', 'Unknown')}: {e}", "ERROR")
                continue
        
        self.add_log(f"å¸–å­è¯¦æƒ…çˆ¬å–å®Œæˆï¼ŒæˆåŠŸçˆ¬å– {len(detailed_threads)} ä¸ªå¸–å­")
        return detailed_threads
    
    async def save_results(self, threads: List[Dict], fid: str, theme_name: str):
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{theme_name}_{fid}_{timestamp}.json"
            filepath = self.save_dir / filename
            
            result_data = {
                "metadata": {
                    "theme_name": theme_name,
                    "fid": fid,
                    "crawl_time": datetime.now().isoformat(),
                    "total_threads": len(threads)
                },
                "threads": threads
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)
            
            self.add_log(f"ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
            
        except Exception as e:
            self.add_log(f"ä¿å­˜ç»“æœå¤±è´¥: {e}", "ERROR")
    
    def stop_crawler(self):
        self.is_running = False
        self.add_log("çˆ¬è™«åœæ­¢å‘½ä»¤å·²å‘é€")
    
    async def reload_cookies(self):
        """é‡æ–°åŠ è½½cookies"""
        try:
            await self.http_client.load_cookies()
            self.add_log("cookiesé‡æ–°åŠ è½½æˆåŠŸ")
        except Exception as e:
            self.add_log(f"é‡æ–°åŠ è½½cookieså¤±è´¥: {e}", "ERROR")
    
    async def get_crawler_status(self) -> Dict:
        return {
            "is_running": self.is_running,
            "max_concurrent": self.max_concurrent,
            "proxy": self.proxy,
            "cookies_file": self.cookies_file,
            "save_dir": str(self.save_dir)
        }
    
    def get_available_themes(self) -> Dict:
        return self.parser.themes
    
    async def test_connection(self) -> bool:
        try:
            client = await self.http_client.create_client()
            response = await self.http_client.request_with_semaphore(
                client, "https://sehuatang.org"
            )
            await self.http_client.close_client(client)
            
            if response and response.status_code == 200:
                self.add_log("è¿æ¥æµ‹è¯•æˆåŠŸ")
                return True
            else:
                self.add_log("è¿æ¥æµ‹è¯•å¤±è´¥", "ERROR")
                return False
        except Exception as e:
            self.add_log(f"è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}", "ERROR")
            return False


# ==================== æµ‹è¯•å‡½æ•° ====================
async def test_new_crawler():
    """æµ‹è¯•æ–°çˆ¬è™«ç³»ç»Ÿ"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ–°çˆ¬è™«ç³»ç»Ÿ...")
    
    # åˆ›å»ºçˆ¬è™«æ§åˆ¶å™¨
    crawler = NewCrawlerController(
        max_concurrent=3,
        proxy=None,  # è®¾ç½®ä»£ç†å¦‚ "http://127.0.0.1:7890"
        cookies_file="data/cookies.json"
    )
    
    # æµ‹è¯•è¿æ¥
    print("ğŸ” æµ‹è¯•è¿æ¥...")
    if await crawler.test_connection():
        print("âœ… è¿æ¥æµ‹è¯•æˆåŠŸ")
        
        # æ˜¾ç¤ºå¯ç”¨ä¸»é¢˜
        themes = crawler.get_available_themes()
        print(f"ğŸ“‹ å¯ç”¨ä¸»é¢˜: {len(themes)} ä¸ª")
        for fid, theme in themes.items():
            print(f"  - {fid}: {theme['name']}")
        
        # çˆ¬å–äºšæ´²æ— ç å‰2é¡µ
        print("ğŸ•·ï¸ å¼€å§‹çˆ¬å–æµ‹è¯•...")
        threads = await crawler.crawl_single_theme(
            fid="36",
            start_page=1,
            end_page=2,
            keywords=["ä¸­æ–‡å­—å¹•", "æ— ç "]
        )
        
        print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…± {len(threads)} ä¸ªå¸–å­")
        
        # æ˜¾ç¤ºå‰5ä¸ªç»“æœ
        print("ğŸ“ å‰5ä¸ªç»“æœ:")
        for i, thread in enumerate(threads[:5]):
            print(f"  {i+1}. {thread['title']} (TID: {thread['tid']})")
    else:
        print("âŒ è¿æ¥æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    asyncio.run(test_new_crawler())
